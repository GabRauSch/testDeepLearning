# Harmful comments

This is a python project with the exclusive goal of initiation in deeplearning

The project was made using Jupiter Notebook, so it may cause the readbility of the project to be affected

Also, to understand further all the content present, I fully recoment watching the video <strong>Build a Comment Toxicity Model with Deep Learning and Python </strong>

## Structure

The model for testing is compiled under harm_text.h5 in the src folder

The train data can be downloaded <a href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge">here</a>
click in late submission and extract the content to ./src

To test the model, I recommend using the tests.py, that already contains two samples